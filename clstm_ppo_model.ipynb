{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### file name: clstm_ppo_model.ipynb\n",
    "### description: This notebook contains the implementation of the clstm_ppo_model described in the paper \"A novel Deep Reinforcement Learning based automated stock trading system using cascaded LSTM networks\" by Jie Zou et al.\n",
    "### author: Damiano Pasquini [pasquini.damiano00@gmail.com]\n",
    "### dataset citation: Dong, Z., Fan, X., & Peng, Z. (2024). FNSPID: A Comprehensive Financial News Dataset in Time Series. arXiv preprint arXiv:2402.06698.\n",
    "### license: Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC-4.0) license"
   ],
   "id": "a38e23493b0dd39f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T15:44:11.738154Z",
     "start_time": "2025-02-11T15:44:07.371992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# imports\n",
    "import os, os.path\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import PPO\n",
    "from torchgen.api.types import deviceT\n",
    "\n",
    "from src.StockTradingEnv import StockTradingEnv\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from src.data_processor import DataPreprocessor as dp\n"
   ],
   "id": "4047f5d838ba85f9",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T15:44:11.745677Z",
     "start_time": "2025-02-11T15:44:11.741678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CUDA availability\n",
    "def check_cuda(use_gpu=False):\n",
    "    \"\"\"\n",
    "    This function checks the availability of CUDA and prints the version of PyTorch and CUDA, and the GPU name.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    print(\"PyTorch Version:\", torch.__version__)\n",
    "    print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        if use_gpu:\n",
    "            print(\"CUDA Version:\", torch.version.cuda)\n",
    "            print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "            torch.device(\"cuda\")\n",
    "            os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "            print(\"Using GPU\")\n",
    "        else:\n",
    "            torch.device(\"cpu\")\n",
    "            os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "            print(\"Using CPU\")"
   ],
   "id": "1ebc2e6b352db2ef",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T15:44:11.854994Z",
     "start_time": "2025-02-11T15:44:11.849022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_df(path, num_stocks):\n",
    "    \"\"\"\n",
    "    This function builds a dataframe from the csv files in the path directory, with the first num_stocks.\n",
    "    :param path: path to the directory containing the csv files\n",
    "    :param num_stocks: number of stocks to consider, must be 5, 25 or 50\n",
    "    :return: a dataframe containing the data from the csv files\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise ValueError(\"The specified path does not exist.\")\n",
    "    if not os.path.isdir(path):\n",
    "        raise ValueError(\"The specified path is not a directory.\")\n",
    "    if not os.listdir(path):\n",
    "        raise ValueError(\"The specified directory is empty.\")\n",
    "    if not all([f.endswith('.csv') for f in os.listdir(path)]):\n",
    "        raise ValueError(\"The specified directory contains files that are not CSV files.\")\n",
    "    if not num_stocks == 5 or num_stocks == 25 or num_stocks == 50:\n",
    "        raise ValueError(\"The number of stocks must be 5, 25 or 50.\")\n",
    "    # Test csvs = 50\n",
    "    names_50 = ['aal.csv', 'AAPL.csv', 'ABBV.csv', 'AMD.csv', 'amgn.csv', 'AMZN.csv', 'BABA.csv',\n",
    "                'bhp.csv', 'bidu.csv', 'biib.csv', 'BRK-B.csv', 'C.csv', 'cat.csv', 'cmcsa.csv', 'cmg.csv',\n",
    "                'cop.csv', 'COST.csv', 'crm.csv', 'CVX.csv', 'dal.csv', 'DIS.csv', 'ebay.csv', 'GE.csv',\n",
    "                'gild.csv', 'gld.csv', 'GOOG.csv', 'gsk.csv', 'INTC.csv', 'KO.csv', 'mrk.csv', 'MSFT.csv',\n",
    "                'mu.csv', 'nke.csv', 'nvda.csv', 'orcl.csv', 'pep.csv', 'pypl.csv', 'qcom.csv', 'QQQ.csv',\n",
    "                'SBUX.csv', 'T.csv', 'tgt.csv', 'tm.csv', 'TSLA.csv', 'TSM.csv', 'uso.csv', 'v.csv', 'WFC.csv',\n",
    "                'WMT.csv', 'xlf.csv']\n",
    "\n",
    "    # Test csvs = 25\n",
    "    names_25 = ['AAPL.csv', 'ABBV.csv', 'AMZN.csv', 'BABA.csv', 'BRK-B.csv', 'C.csv', 'COST.csv', 'CVX.csv',\n",
    "                'DIS.csv', 'GE.csv', 'INTC.csv', 'MSFT.csv', 'nvda.csv', 'pypl.csv', 'QQQ.csv', 'SBUX.csv', 'T.csv',\n",
    "                'TSLA.csv', 'WFC.csv', 'KO.csv', 'AMD.csv', 'TSM.csv', 'GOOG.csv', 'WMT.csv']\n",
    "\n",
    "    # Test csvs = 5\n",
    "    names_5 = ['KO.csv', 'AMD.csv', 'TSM.csv', 'GOOG.csv', 'WMT.csv']\n",
    "    df = pd.DataFrame()\n",
    "    stocks = names_5 if num_stocks == 5 else names_25 if num_stocks == 25 else names_50\n",
    "    for stock in stocks:\n",
    "        stock = pd.read_csv(path + '/' + stock)\n",
    "        df = pd.concat([df, stock])\n",
    "    df.sort_values(by='Date', inplace=True)\n",
    "    return df"
   ],
   "id": "42bda000ebfeb12e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T15:44:11.865973Z",
     "start_time": "2025-02-11T15:44:11.860804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def feature_transformation(df):\n",
    "    \"\"\"\n",
    "    This function is used to execute all the preprocessing steps over the features contained in the dataframe.\n",
    "    These transformations must be applied only to train and validation sets, not to the test set.\n",
    "    :param df: the dataframe\n",
    "    :return: the transformed dataframe\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # Convert Date to datetime and set as index\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "    df.set_index(\"Date\", inplace=True)\n",
    "    # Handle missing values\n",
    "    df.ffill(inplace=True)\n",
    "    # Standardize the data with StandardScaler (z-score)\n",
    "    scaler = StandardScaler()\n",
    "    cols_to_scale = ['Open', 'High', 'Low', 'Close', 'Adj close', 'Volume', 'Sentiment_gpt', 'Scaled_sentiment']\n",
    "    for col in cols_to_scale:\n",
    "        if col in df.columns:\n",
    "            df[col] = scaler.fit_transform(df[col].values.reshape(-1, 1))\n",
    "    # df[cols_to_scale] = scaler.fit_transform(df[cols_to_scale])\n",
    "    # Normalize numerical columns\n",
    "    scaler = MinMaxScaler()\n",
    "    cols_to_normalize = [\"Open\", \"High\", \"Low\", \"Close\", 'Adj close', \"Volume\", \"Sentiment_gpt\", \"Scaled_sentiment\"]\n",
    "    df[cols_to_normalize] = scaler.fit_transform(df[cols_to_normalize])\n",
    "    print('Dataset preprocessing completed (standardization and normalization).')\n",
    "    return df"
   ],
   "id": "63b26cd2e3da4652",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T15:44:11.876516Z",
     "start_time": "2025-02-11T15:44:11.872679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def split_data(df, test_size=0.10, val_size=0.10):\n",
    "    \"\"\"\n",
    "    Split the data into train, validation and test sets.\n",
    "    :param df: the dataframe\n",
    "    :param test_size: the size of the test set\n",
    "    :param val_size: the size of the validation set\n",
    "    :return: train, test, and validation sets\n",
    "    \"\"\"\n",
    "    train_size = int(df.shape[0] * (1 - test_size - val_size))\n",
    "    train = df[:train_size]\n",
    "    test_size = int(df.shape[0] * test_size)\n",
    "    test = df[train_size:train_size + test_size]\n",
    "    val_size = int(df.shape[0] * val_size)\n",
    "    val = df[train_size + test_size:train_size + test_size + val_size]\n",
    "    return train, test, val"
   ],
   "id": "60ff384e772c301",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T15:44:11.886547Z",
     "start_time": "2025-02-11T15:44:11.882547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CLSTM-PPO model\n",
    "\n",
    "# Define LSTM-based feature extractor\n",
    "class LSTMFeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    This class defines the LSTM-based feature extractor used in the CLSTM-PPO model.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTMFeatureExtractor, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        This function performs the forward pass of the feature extractor.\n",
    "        :param x: input tensor\n",
    "        :return: features\n",
    "        \"\"\"\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        features = self.fc(lstm_out[:, -1, :])\n",
    "        return features\n",
    "\n",
    "# Define PPO Model with LSTM feature extractor\n",
    "class CLSTM_PPO(nn.Module):\n",
    "    \"\"\"\n",
    "    This class defines the CLSTM-PPO model.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(CLSTM_PPO, self).__init__()\n",
    "        self.feature_extractor = LSTMFeatureExtractor(state_dim, hidden_dim, hidden_dim)\n",
    "        self.actor = nn.Linear(hidden_dim, action_dim)\n",
    "        self.critic = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        This function performs the forward pass of the model.\n",
    "        :param state: input tensor\n",
    "        :return: action_probs, value\n",
    "        \"\"\"\n",
    "        features = self.feature_extractor(state)\n",
    "        action_probs = torch.softmax(self.actor(features), dim=-1)\n",
    "        value = self.critic(features)\n",
    "        return action_probs, value"
   ],
   "id": "79fe17d4f28485bb",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T15:44:11.897141Z",
     "start_time": "2025-02-11T15:44:11.891631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    # Load data\n",
    "    dir_path = 'dataset/processed/data_for_lstm'\n",
    "    if not os.path.exists(dir_path):\n",
    "        raise ValueError(\"The specified path does not exist.\")\n",
    "    # CUDA availability (set True to use GPU)\n",
    "    check_cuda(False)\n",
    "    # Build dataframe\n",
    "    data_frame = build_df(dir_path, 5)\n",
    "    # Feature transformation\n",
    "    data_frame = feature_transformation(data_frame)\n",
    "    # Split data\n",
    "    train_data, test_data, val_data = split_data(data_frame)\n",
    "    # Environment Setup\n",
    "    env = DummyVecEnv([lambda: StockTradingEnv(train_data)])\n",
    "    # PPO model setup, to be run on CPU\n",
    "    model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./logs/ppo_logs/\")\n",
    "    # Training loop\n",
    "    n_steps = 10000\n",
    "    for i in range(n_steps):\n",
    "        model.learn(total_timesteps=1000, reset_num_timesteps=False)\n",
    "        if i % 10 == 0:\n",
    "            model.save(f\"./models/CLSTM_PPO_SENTIMENT/checkpoints/ppo_stock_{i}.zip\")\n",
    "            print(f\"Checkpoint saved at step {i}\")\n",
    "    # Save final model\n",
    "    model.save(\"./models/CLSTM_PPO_SENTIMENT/ppo_stock_final.zip\")"
   ],
   "id": "aa773dabd712874d",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T15:44:12.347902Z",
     "start_time": "2025-02-11T15:44:11.903784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "id": "ffde3b81f94f28fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.6.0+cu126\n",
      "CUDA Available: True\n",
      "Using CPU\n",
      "Dataset preprocessing completed (standardization and normalization).\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'sort_values'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m----> 2\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[7], line 15\u001B[0m, in \u001B[0;36mmain\u001B[1;34m()\u001B[0m\n\u001B[0;32m     13\u001B[0m train_data, test_data, val_data \u001B[38;5;241m=\u001B[39m split_data(data_frame)\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# Environment Setup\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m env \u001B[38;5;241m=\u001B[39m \u001B[43mDummyVecEnv\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mStockTradingEnv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# PPO model setup, to be run on CPU\u001B[39;00m\n\u001B[0;32m     17\u001B[0m model \u001B[38;5;241m=\u001B[39m PPO(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMlpPolicy\u001B[39m\u001B[38;5;124m\"\u001B[39m, env, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, tensorboard_log\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./logs/ppo_logs/\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\time_series_forecasting\\venv\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:31\u001B[0m, in \u001B[0;36mDummyVecEnv.__init__\u001B[1;34m(self, env_fns)\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, env_fns: \u001B[38;5;28mlist\u001B[39m[Callable[[], gym\u001B[38;5;241m.\u001B[39mEnv]]):\n\u001B[1;32m---> 31\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menvs \u001B[38;5;241m=\u001B[39m [_patch_env(\u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m) \u001B[38;5;28;01mfor\u001B[39;00m fn \u001B[38;5;129;01min\u001B[39;00m env_fns]\n\u001B[0;32m     32\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mset\u001B[39m([\u001B[38;5;28mid\u001B[39m(env\u001B[38;5;241m.\u001B[39munwrapped) \u001B[38;5;28;01mfor\u001B[39;00m env \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menvs])) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menvs):\n\u001B[0;32m     33\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m     34\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou tried to create multiple environments, but the function to create them returned the same instance \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     35\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minstead of creating different objects. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     40\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease read https://github.com/DLR-RM/stable-baselines3/issues/1151 for more information.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     41\u001B[0m         )\n",
      "Cell \u001B[1;32mIn[7], line 15\u001B[0m, in \u001B[0;36mmain.<locals>.<lambda>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     13\u001B[0m train_data, test_data, val_data \u001B[38;5;241m=\u001B[39m split_data(data_frame)\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# Environment Setup\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m env \u001B[38;5;241m=\u001B[39m DummyVecEnv([\u001B[38;5;28;01mlambda\u001B[39;00m: \u001B[43mStockTradingEnv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[43m)\u001B[49m])\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# PPO model setup, to be run on CPU\u001B[39;00m\n\u001B[0;32m     17\u001B[0m model \u001B[38;5;241m=\u001B[39m PPO(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMlpPolicy\u001B[39m\u001B[38;5;124m\"\u001B[39m, env, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, tensorboard_log\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./logs/ppo_logs/\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\time_series_forecasting\\src\\StockTradingEnv.py:35\u001B[0m, in \u001B[0;36mStockTradingEnv.__init__\u001B[1;34m(self, datasets, window_size, initial_capital, trade_amount)\u001B[0m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstock_data \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m df \u001B[38;5;129;01min\u001B[39;00m datasets:\n\u001B[1;32m---> 35\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msort_values\u001B[49m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDate\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     36\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstock_data\u001B[38;5;241m.\u001B[39mappend(df[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeature_cols]\u001B[38;5;241m.\u001B[39mto_numpy())\n\u001B[0;32m     38\u001B[0m \u001B[38;5;66;03m# Ensure all stocks have the same number of timesteps\u001B[39;00m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'str' object has no attribute 'sort_values'"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
